# -*- coding: utf-8 -*-
"""senai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bcVQzq2WhVJWQxOT5AhJbZ28XeCnJ3fW
"""



"""# Desafio:

Uma empresa do setor siderúrgico contratou você para a criação de um sistema inteligente
de controle de qualidade para chapas de aço inoxidável. Essa empresa forneceu um conjunto
de dados contendo informações extraídas a partir de imagens de superfície das chapas, com
o objetivo de detectar e classificar defeitos automaticamente. Cada amostra no conjunto de
dados é composta por 31 indicadores que descrevem aspectos geométricos e estatísticos
do defeito identificado, como área, perímetro, índices de orientação, luminosidade e
proporção de bordas. Além dessas características, cada amostra é rotulada com uma das
sete possíveis classes de defeitos (seis categorias específicas e uma categoria genérica de
“outros defeitos”).
O sistema deverá ser capaz de, a partir do cadastro de uma nova imagem (ou conjunto de
medições), prever a classe do defeito e retornar a probabilidade associada. Além disso, a
empresa espera que você extraia insights da operação e dos defeitos e gere visualizações de
dados.

## 01 Planejamento

### Minha Lista de Tarefas

- [x] **01 Planejamento**
    - [x] Criar lista inicial de tarefas (Geral)
- [x] **02 Definição do Problema de Negócio**
    - [x] Definir os objetivos do projeto.
    - [x] Identificar as necessidades da empresa.
- [x] **03 Entendimento dos Dados**
    - [x] Compreender a estrutura dos dados e as colunas (features).
    - [x] Explorar estatísticas descritivas básicas.
    - [x] Verificar o balanceamento das classes de defeito (variável alvo).
- [x] **04 Preparação dos Dados**
    - [x] Realizar o pré-processamento dos dados (tratamento de valores ausentes, se houver).
- [x] **05 Modelagem**
    - [x] Selecionar um modelo de classificação simples (e.g., Regressão Logística).
    - [x] Dividir os dados em conjuntos de treino e teste.
    - [x] Treinar o modelo selecionado.
    - [x] Avaliar o desempenho do modelo (acurácia, precisão, recall, F1-score, etc.).
    - [x] Implementar uma função simples para realizar predições com probabilidades.
relevantes com cores representando as classes.
- [x] **06 Engenharia de Features**
    - [x] Criar novas variáveis (features) a partir dos dados existentes ou transformar as variáveis existentes
- [x] **07 Definição de Modelos**
    - [x] Testar diferentes modelos e comparar seus desempenhos.
    - [X] Documentar brevemente os modelos e os principais insights.
- [X] **08 Preparar um resumo dos resultados.**

## 02 Definição do Problema de Negócio

### 2.1 Definir os objetivos do projeto.

Os principais objetivos deste projeto são:
* Desenvolver um sistema inteligente de controle de qualidade: Criar uma solução de aprendizado de máquina capaz de analisar dados de imagens de superfície de chapas de aço inoxidável para detectar e classificar defeitos automaticamente.
* Prever a classe de defeito: Implementar um modelo de classificação que, a partir dos 31 indicadores fornecidos para uma nova amostra (ou conjunto de medições), seja capaz de prever a qual das sete classes de defeito ela pertence (seis específicas e "outros defeitos").
* Retornar a probabilidade associada: Para cada previsão de classe de defeito, o sistema deve fornecer a probabilidade de pertencimento àquela classe, oferecendo uma medida de confiança na predição.
* Extrair insights da operação e dos defeitos: Analisar os dados fornecidos para identificar padrões, relações e características importantes dos diferentes tipos de defeitos, bem como possíveis insights sobre o processo de produção que possam estar relacionados à ocorrência desses defeitos.
* Gerar visualizações de dados: Criar representações gráficas dos dados e dos resultados da análise para facilitar a compreensão dos defeitos, sua distribuição, as características relevantes e o desempenho do modelo de classificação.

### 2.2 Identificar as necessidades da empresa

As necessidades da empresa são:
* Automatização do controle de qualidade: Reduzir ou eliminar a necessidade de inspeção manual das chapas de aço inoxidável, tornando o processo mais rápido, eficiente e potencialmente mais preciso.
* Detecção precoce de defeitos: Identificar defeitos nas chapas em um estágio inicial do processo, o que pode levar a economias ao evitar o processamento adicional de materiais defeituosos.
* Classificação precisa de defeitos: Categorizar os defeitos em classes específicas para permitir uma melhor compreensão das causas e a implementação de ações corretivas direcionadas para cada tipo de problema.
* Informações para tomada de decisão: Obter insights valiosos sobre a ocorrência e as características dos defeitos para auxiliar na otimização do processo de produção, na melhoria da qualidade do produto e na redução de perdas.
* Ferramenta de análise e visualização: Dispor de uma ferramenta que permita visualizar a distribuição dos defeitos, as relações entre as variáveis e o desempenho do sistema de classificação, facilitando a comunicação e a compreensão dos resultados por diferentes áreas da empresa.
* Previsão para novas amostras: Ter a capacidade de analisar novas imagens ou medições e obter uma previsão da classe de defeito com a respectiva probabilidade, permitindo a aplicação do sistema em tempo real no processo de controle de qualidade.

## 03 Entendimento dos Dados

### 3.1 Compreender a estrutura dos dados e as colunas

| Campo                   | Descrição                                                                                                                               |
|-------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| `id`                    | Identificador das amostras do banco.                                                                                                     |
| `x_minimo`              | A menor coordenada X da *bounding box* ao redor de um defeito ou característica na placa de aço.                                      |
| `x_maximo`              | A maior coordenada X da *bounding box* ao redor de um defeito ou característica na placa de aço.                                       |
| `y_minimo`              | A menor coordenada Y da caixa delimitadora ao redor de um defeito ou característica na placa de aço.                                   |
| `y_maximo`              | A maior coordenada Y da caixa delimitadora ao redor de um defeito ou característica na placa de aço.                                    |
| `peso_da_placa`         | Peso da placa.                                                                                                                          |
| `area_pixels`           | O número total de pixels dentro da região do defeito ou característica.                                                               |
| `perimetro_x`           | O perímetro (comprimento da borda) do defeito ou característica ao longo do eixo X.                                                    |
| `perimetro_y`           | O perímetro (comprimento da borda) do defeito ou característica ao longo do eixo Y.                                                    |
| `soma_da_luminosidade`  | A soma dos valores de luminosidade (brilho) dos pixels dentro da região do defeito ou característica.                                  |
| `minimo_da_luminosidade`| O menor valor de luminosidade dentro da região do defeito ou característica.                                                            |
| `maximo_da_luminosidade`| O maior valor de luminosidade dentro da região do defeito ou característica.                                                            |
| `comprimento_do_transportador` | Comprimento da esteira ou plataforma sobre a qual as chapas de aço são transportadas.                                              |
| `tipo_do_aço_A300`      | Atributo binário que indica se a chapa de aço é do tipo A300.                                                                           |
| `tipo_do_aço_A400`      | Atributo binário que indica se a chapa de aço é do tipo A400.                                                                           |
| `temperatura`           | Temperatura do processo.                                                                                                                |
| `espessura_da_chapa_de_aço`| Espessura da chapa de aço.                                                                                                             |
| `index_de_bordas`       | Índice que indica a razão entre o comprimento das bordas e o comprimento total da borda do defeito ou característica.                    |
| `index_vazio`           | Índice que indica a razão entre os espaços vazios (fundo) e a área total do defeito ou característica.                                   |
| `index_quadrado`        | Índice que indica a razão entre a área do defeito ou característica e a área do menor quadrado que a envolve.                           |
| `index_externo_x`       | Índice que indica a razão entre os pixels fora do defeito ou característica ao longo do eixo X e a área total.                          |
| `indice_de_bordas_x`    | Índice que indica a razão entre os pixels da borda ao longo do eixo X e o número total de pixels de borda.                               |
| `indice_de_bordas_y`    | Índice que indica a razão entre os pixels da borda ao longo do eixo Y e o número total de pixels de borda.                               |
| `indice_de_variacao_x`  | Índice que mede a dispersão das coordenadas X dos pixels de contorno do defeito.                                                       |
| `indice_de_variacao_y`  | Índice que mede a dispersão das coordenadas Y dos pixels de contorno do defeito.                                                       |
| `indice_global_externo` | Índice que indica a razão entre os pixels fora do defeito ou característica e o número total de pixels da imagem.                        |
| `log_das_areas`         | Logaritmo da área do defeito ou característica.                                                                                         |
| `log_indice_x`          | Índice que indica o logaritmo da razão entre o comprimento das bordas ao longo do eixo X e o comprimento total das bordas.              |
| `log_indice_y`          | Índice que indica o logaritmo da razão entre o comprimento das bordas ao longo do eixo Y e o comprimento total das bordas.              |
| `indice_de_orientaçao`  | Índice que indica a orientação do defeito ou característica dentro da imagem.                                                          |
| `indice_de_luminosidade`| Índice que indica o contraste de luminosidade entre o defeito ou característica e seu fundo.                                            |
| `sigmoide_das_areas`    | Transformação sigmoide da área do defeito ou característica.                                                                           |
| `falha_1`               | Tipos específicos de defeitos.                                                                                                        |
| `falha_2`               | Tipos específicos de defeitos.                                                                                                        |
| `falha_3`               | Tipos específicos de defeitos.                                                                                                        |
| `falha_4`               | Tipos específicos de defeitos.                                                                                                        |
| `falha_5`               | Tipos específicos de defeitos.                                                                                                        |
| `falha_6`               | Tipos específicos de defeitos.                                                                                                        |
| `falha_outros`          | Outros tipos de defeito não abrangidos nas classificações anteriores.                                                                   |
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

lista_observacao = []

lista_observacao

def coletar_observacoes():
    """
    Permite ao usuário inserir diversas observações, armazenando-as em uma lista.

    Retorna:
        list: Uma lista contendo todas as observações inseridas pelo usuário.
    """

    print("Inicie a inserção das suas observações. Para finalizar, digite 'fim'.")
    while True:
        observacao = input("> ")
        if observacao.lower() == 'fim':
            break
        lista_observacao.append(observacao)
    print("\nObservações coletadas com sucesso!")
    return lista_observacao

# Exemplo de uso da função:
minhas_observacoes = coletar_observacoes()
print("\nLista de observações armazenadas:")
print(minhas_observacoes)

minhas_observacoes = coletar_observacoes()

# Caminhos para os arquivos CSV
train_file = '/content/bootcamp_train.csv'
test_file = '/content/bootcamp_test.csv'

# Leitura do dataset de treino
try:
    df_train = pd.read_csv(train_file)
    print("Dataset de treino carregado com sucesso!")
    print(f"Número de linhas e colunas: {df_train.shape}")
    print("\nPrimeiras 5 linhas do dataset de treino:")
    print(df_train.head())
except FileNotFoundError:
    print(f"Erro: O arquivo '{train_file}' não foi encontrado.")
except Exception as e:
    print(f"Ocorreu um erro ao ler o arquivo de treino: {e}")

# Leitura do dataset de teste
try:
    df_test = pd.read_csv(test_file)
    print("\nDataset de teste carregado com sucesso!")
    print(f"Número de linhas e colunas: {df_test.shape}")
    print("\nPrimeiras 5 linhas do dataset de teste:")
    print(df_test.head())
except FileNotFoundError:
    print(f"Erro: O arquivo '{test_file}' não foi encontrado.")
except Exception as e:
    print(f"Ocorreu um erro ao ler o arquivo de teste: {e}")

df_train.head(5)

df_train.columns

try:
    print("Dataset de treino carregado com sucesso!\n")

    # Iterando sobre as colunas e imprimindo as 5 primeiras linhas de cada uma
    for col in df_train.columns:
        print(f"Coluna: {col}")
        print(df_train[col].head())
        print("-" * 30)  # Adiciona uma linha separadora para melhor visualização

except FileNotFoundError:
    print(f"Erro: O arquivo '{train_file}' não foi encontrado.")
except Exception as e:
    print(f"Ocorreu um erro ao ler o arquivo de treino: {e}")

df_train.info()

df_test.info() # Padronizar se realizar alguma alteração

"""### 3.2 Explorar estatísticas descritivas básicas."""

df_train.describe()

# Estatísticas básicas para a coluna 'id'

print(f"Estatísticas básicas da coluna 'id':")
print(f"  Valor Máximo: {df_train['id'].max()}")
print(f"  Valor Mínimo: {df_train['id'].min()}")
print(f"  Valores em Branco (NaN): {df_train['id'].isnull().sum()}")
print(f"  Valores Nulos (None): {df_train['id'].isna().sum()}") # Equivalente a isnull para detecção
print(f"  Número de valores duplicados: {df_train['id'].duplicated().sum()}")

# Estatísticas básicas para a coluna 'x_minimo'
print(f"Estatísticas básicas da coluna 'x_minimo':")
print(f"  Média: {df_train['x_minimo'].mean()}")
print(f"  Moda: {df_train['x_minimo'].mode().tolist()}") # A moda pode ter múltiplos valores
print(f"  Valor Máximo: {df_train['x_minimo'].max()}")
print(f"  Valor Mínimo: {df_train['x_minimo'].min()}")
print(f"  Valores em Branco (NaN): {df_train['x_minimo'].isnull().sum()}")
print(f"  Valores Nulos (None): {df_train['x_minimo'].isna().sum()}") # Equivalente a isnull para detecção abrangente
print(f"  Valores Únicos: {df_train['x_minimo'].nunique()}")
print(f"  Desvio Padrão: {df_train['x_minimo'].std()}")
print(f"  Número de valores duplicados: {df_train['x_minimo'].duplicated().sum()}")

import seaborn as sns
import matplotlib.pyplot as plt



plt.figure(figsize=(8, 6))  # Define o tamanho da figura
sns.boxplot(x=df_train['x_minimo'])
plt.title('Box Plot da Coluna "x_minimo"')
plt.xlabel('x_minimo')
plt.show()

minhas_observacoes = coletar_observacoes()

def analisar_coluna(dataframe, nome_coluna):
    """
    Calcula e exibe estatísticas básicas, valores duplicados e gera um box plot para uma coluna específica de um DataFrame.

    """
    if nome_coluna not in dataframe.columns:
        print(f"Erro: A coluna '{nome_coluna}' não existe no DataFrame.")
        return

    print(f"\nEstatísticas básicas para a coluna '{nome_coluna}':")
    print(f"  Média: {dataframe[nome_coluna].mean()}")
    print(f"  Moda: {dataframe[nome_coluna].mode().tolist()}")
    print(f"  Valor Máximo: {dataframe[nome_coluna].max()}")
    print(f"  Valor Mínimo: {dataframe[nome_coluna].min()}")
    print(f"  Valores em Branco (NaN): {dataframe[nome_coluna].isnull().sum()}")
    print(f"  Valores Nulos (None): {dataframe[nome_coluna].isna().sum()}")
    print(f"  Valores Únicos: {dataframe[nome_coluna].nunique()}")
    print(f"  Desvio Padrão: {dataframe[nome_coluna].std()}")
    print(f"  Número de valores duplicados: {dataframe[nome_coluna].duplicated().sum()}")

    # Gerar Box Plot
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=dataframe[nome_coluna])
    plt.title(f'Box Plot da Coluna "{nome_coluna}"')
    plt.xlabel(nome_coluna)
    plt.show()
    # Exemplo de uso da função para a coluna 'x_minimo' do df_train
    # analisar_coluna(df_train, 'x_minimo')

def analisar_coluna(dataframe, nome_coluna):
    """
    Calcula e exibe estatísticas básicas, valores duplicados e gera um box plot para uma coluna específica de um DataFrame.


    """
    if nome_coluna not in dataframe.columns:
        print(f"Erro: A coluna '{nome_coluna}' não existe no DataFrame.")
        return

    print(f"\nAnálise da coluna '{nome_coluna}':")
    print(f"  Tipo de dados: {dataframe[nome_coluna].dtype}")
    print(f"  Número de valores não nulos: {dataframe[nome_coluna].count()}")
    print(f"  Número de valores únicos: {dataframe[nome_coluna].nunique()}")
    print(f"  Número de valores duplicados: {dataframe[nome_coluna].duplicated().sum()}")
    print(f"  Número de valores em branco (NaN): {dataframe[nome_coluna].isnull().sum()}")
    print(f"  Número de valores nulos (None): {dataframe[nome_coluna].isna().sum()}")

    if pd.api.types.is_numeric_dtype(dataframe[nome_coluna]):
        print(f"  Média: {dataframe[nome_coluna].mean()}")
        print(f"  Mediana: {dataframe[nome_coluna].median()}")
        print(f"  Moda: {dataframe[nome_coluna].mode().tolist()}")
        print(f"  Valor Máximo: {dataframe[nome_coluna].max()}")
        print(f"  Valor Mínimo: {dataframe[nome_coluna].min()}")
        print(f"  Desvio Padrão: {dataframe[nome_coluna].std()}")

        # Gerar Box Plot para colunas numéricas
        import seaborn as sns
        import matplotlib.pyplot as plt

        plt.figure(figsize=(8, 6))
        sns.boxplot(x=dataframe[nome_coluna])
        plt.title(f'Box Plot da Coluna "{nome_coluna}"')
        plt.xlabel(nome_coluna)
        plt.show()
    else:
        print("  A coluna não é numérica, o Box Plot não será gerado.")
        print(f"  Valores mais frequentes:\n{dataframe[nome_coluna].value_counts().head()}")
    #minhas_observacoes = coletar_observacoes()

analisar_coluna(df_train, 'x_maximo')

minhas_observacoes = coletar_observacoes()

analisar_coluna(df_train, 'y_minimo')

minhas_observacoes = coletar_observacoes()

analisar_coluna(df_train, 'y_maximo')

minhas_observacoes = coletar_observacoes()

analisar_coluna(df_train, 'peso_da_placa')

analisar_coluna(df_train, 'area_pixels')

analisar_coluna(df_train, 'perimetro_x')

analisar_coluna(df_train, 'perimetro_y')

analisar_coluna(df_train, 'soma_da_luminosidade')

analisar_coluna(df_train, 'maximo_da_luminosidade')

analisar_coluna(df_train, 'comprimento_do_transportador')

analisar_coluna(df_train, 'tipo_do_aço_A300')

analisar_coluna(df_train, 'tipo_do_aço_A400')

analisar_coluna(df_train, 'espessura_da_chapa_de_aço')

analisar_coluna(df_train, 'temperatura')

analisar_coluna(df_train, 'index_de_bordas')

analisar_coluna(df_train, 'index_vazio')

analisar_coluna(df_train, 'index_quadrado')

analisar_coluna(df_train, 'index_externo_x')

analisar_coluna(df_train, 'indice_de_bordas_x')

analisar_coluna(df_train, 'indice_de_bordas_y')

analisar_coluna(df_train, 'indice_de_variacao_x')

analisar_coluna(df_train, 'indice_de_variacao_y')

analisar_coluna(df_train, 'indice_global_externo')

analisar_coluna(df_train, 'log_das_areas')

analisar_coluna(df_train, 'log_indice_x')

analisar_coluna(df_train, 'log_indice_y')

analisar_coluna(df_train, 'indice_de_orientaçao')

analisar_coluna(df_train, 'indice_de_luminosidade')

analisar_coluna(df_train, 'sigmoide_das_areas')

analisar_coluna(df_train, 'minimo_da_luminosidade')

analisar_coluna(df_train, 'falha_1')

analisar_coluna(df_train, 'falha_2')

analisar_coluna(df_train, 'falha_3')

analisar_coluna(df_train, 'falha_4')

analisar_coluna(df_train, 'falha_5')

analisar_coluna(df_train, 'falha_6')

analisar_coluna(df_train, 'falha_outros')

lista_observacao # Lista de observação

"""**Lista de observação**

* 'x_minimo: Outliers encontrados.',
* 'x_maximo: Valores em branco (NaN): 56. Valores nulos (None): 56.',
* 'y_minimo: Outliers encontrados.',
* 'y_maximo: Outliers.',
* 'peso_da_placa: A coluna tem o mesmo valor (100) para todas as linhas.',
* 'area_pixels: Outliers.',
* 'perimetro_x: Outliers.',
* 'perimetro_y: Outliers.',
* 'soma_da_luminosidade: Outliers. Número de valores em branco (NaN): 100. Número de valores nulos (None): 100.',
* 'maximo_da_luminosidade: Outliers. Número de valores em branco (NaN): 98. Número de valores nulos (None): 98.',
* 'comprimento_do_transportador: Outliers.',
* 'tipo_do_aço_A300: Realizar a padronização dos dados e verificar se precisa padronizar o conjunto de teste. Não: 1169. Sim: 802. Não: 770. Sim: 537. N: 40.',
* 'tipo_do_aço_A400: Padronizar os dados e verificar se precisa padronizar o conjunto de teste. Número de valores em branco (NaN): 76. Número de valores nulos (None): 76.',
* 'tipo_do_aço_A400: Sim: 1142. Não: 772. Sim: 757. Não: 513. S: 40.',
* 'espessura_da_chapa_de_aço: Número de valores em branco (NaN): 41. Número de valores nulos (None): 41 e valor negativo.',
* 'comprimento_do_transportador: Valor negativo.',
* 'temperatura: Outliers.',
* 'index_vazio: Outliers.',
* 'index_quadrado: Número de valores em branco (NaN): 36. Número de valores nulos (None): 36.',
* 'index_externo_x: Outliers.',
* 'index_de_bordas: Número de valores duplicados: 2043.',
* 'indice_de_bordas_x: Número de valores duplicados: 2612.',
* 'indice_de_bordas_y: Outliers. Número de valores duplicados: 2772.',
* 'indice_de_variacao_x: Outliers.',
* 'indice_de_variacao_y: Outliers. Número de valores únicos: 3390.',
* 'indice_global_externo: Número de valores em branco (NaN): 59. Número de valores nulos (None): 59.',
* 'log_das_areas: Outliers.',
* 'log_indice_x: Outliers.',
* 'log_indice_y: Outliers.',
* 'indice_de_orientaçao: Média: 0.09915318584070797. Mediana: 0.1111. Valor Máximo: 0.9917. Valor Mínimo: -0.991. Moda: [0.0]. Verificar.',
* 'indice_de_luminosidade: Outliers. Número de valores em branco (NaN): 50. Número de valores nulos (None): 50.',
* 'sigmoide_das_areas: Pesquisar.',
* 'minimo_da_luminosidade: Outliers.',
* 'falha_1: Padronizar os dados. False: 2992. True: 265. Não: 62. 0: 61. 1: 5.',
* 'falha_2: Padronizar os dados. False: 3064. True: 249. 0: 62. y: 5. 1: 5.',
* 'falha_3: Tipo de dados: bool.',
* 'falha_4: Padronizar. True: 112. Não: 65. 0: 64. S: 2.',
* 'falha_5: Padronizar. Não: 1977. Não: 1317. Sim: 58. Sim: 38.',
* 'falha_6: Tipo de dados: bool.',
* 'falha_outros: Padronizar. Não: 2206. Sim: 1184.'

**Lista de observação da Análise Exploratória Inicial do Dataset**

Esta lista resume as primeiras observações da análise exploratória dos dados, destacando a presença de outliers em diversas colunas numéricas, valores faltantes (NaN e None) em várias características, a necessidade de padronização para colunas categóricas com inconsistências e a identificação de colunas com valores únicos ou duplicados que podem requerer investigação adicional.

### 3.3 Verificar o balanceamento das classes de defeito (variável alvo).
"""

target_columns = ['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros']

print("Balanceamento das classes de defeito:")
for col in target_columns:
    if col in df_train.columns:
        print(f"\n--- Balanceamento para a coluna '{col}' ---")
        print(df_train[col].value_counts())
        print(f"\nProporções para '{col}':")
        print(df_train[col].value_counts(normalize=True))

        # Visualização em gráfico de barras
        plt.figure(figsize=(6, 4))
        sns.countplot(data=df_train, y=col)
        plt.title(f'Distribuição da Classe: {col}')
        plt.xlabel('Número de Ocorrências')
        plt.ylabel('Classe')
        plt.show()
    else:
        print(f"A coluna '{col}' não foi encontrada no DataFrame.")

"""Padronização para cada coluna"""

#Padronização para cada coluna

target_columns = ['falha_1', 'falha_2', 'falha_4', 'falha_5', 'falha_outros']

for col in target_columns:
    if col in df_train.columns:
        # Converter para string para garantir que as comparações funcionem
        df_train[col] = df_train[col].astype(str).str.lower()

        # Mapear todas as variações de verdadeiro para True
        df_train[col] = df_train[col].replace(['true', 'sim', '1', 's', 'y'], True)

        # Mapear todas as variações de falso para False
        df_train[col] = df_train[col].replace(['false', 'não', 'nao', '0', 'n'], False)

        # Converter a coluna para booleano (isso lidará com quaisquer valores que não foram mapeados)
        df_train[col] = df_train[col].astype(bool)

# 'falha_3' e 'falha_6' já são booleanas

if 'falha_3' in df_train.columns:
    df_train['falha_3'] = df_train['falha_3'].astype(bool)
if 'falha_6' in df_train.columns:
    df_train['falha_6'] = df_train['falha_6'].astype(bool)


# Agora, vamos verificar o balanceamento novamente após a padronização corrigida
print("\nBalanceamento das classes de defeito APÓS A PADRONIZAÇÃO (CORRIGIDA):")
for col in target_columns + ['falha_3', 'falha_6']:
    if col in df_train.columns:
        print(f"\n--- Balanceamento para a coluna '{col}' ---")
        print(df_train[col].value_counts())
        print(f"\nProporções para '{col}':")
        print(df_train[col].value_counts(normalize=True))

        # Visualização em gráfico de barras após padronização
        plt.figure(figsize=(6, 4))
        sns.countplot(data=df_train, y=col)
        plt.title(f'Distribuição da Classe (Padr.): {col}')
        plt.xlabel('Número de Ocorrências')
        plt.ylabel('Classe')
        plt.show()
    else:
        print(f"A coluna '{col}' não foi encontrada no DataFrame.")

"""## 04 Preparação dos Dados

**Colunas tipo_do_aço_A300 e tipo_do_aço_A400**
"""

target_columns = ['tipo_do_aço_A300', 'tipo_do_aço_A400']

print("Balanceamento das classes de tipo de aço:")
for col in target_columns:
    if col in df_train.columns:
        print(f"\n--- Balanceamento para a coluna '{col}' ---")
        print(df_train[col].value_counts())
        print(f"\nProporções para '{col}':")
        print(df_train[col].value_counts(normalize=True))

        # Visualização em gráfico de barras
        plt.figure(figsize=(6, 4))
        sns.countplot(data=df_train, y=col)
        plt.title(f'Distribuição da Classe: {col}')
        plt.xlabel('Número de Ocorrências')
        plt.ylabel('Classe')
        plt.show()
    else:
        print(f"A coluna '{col}' não foi encontrada no DataFrame.")

# Remover as linhas onde 'tipo_do_aço_A300' é igual a '-'
df_train = df_train[df_train['tipo_do_aço_A300'] != '-']

#Padronização para cada coluna

target_columns = ['tipo_do_aço_A300', 'tipo_do_aço_A400']


for col in target_columns:
    if col in df_train.columns:
        # Converter para string para garantir que as comparações funcionem
        df_train[col] = df_train[col].astype(str).str.lower()

        # Mapear todas as variações de verdadeiro para True
        df_train[col] = df_train[col].replace(['true','Sim', 'sim', '1','S', 's', 'y'],
         True)

        # Mapear todas as variações de falso para False
        df_train[col] = df_train[col].replace(['false', 'não', 'nao', '0', 'n','N'], False)

        # Converter a coluna para booleano (isso lidará com quaisquer valores que não foram mapeados)
        df_train[col] = df_train[col].astype(bool)



# Agora, vamos verificar o balanceamento novamente após a padronização corrigida
print("\nBalanceamento das classes tipo de aço APÓS A PADRONIZAÇÃO (CORRIGIDA):")
for col in target_columns:
    if col in df_train.columns:
        print(f"\n--- Balanceamento para a coluna '{col}' ---")
        print(df_train[col].value_counts())
        print(f"\nProporções para '{col}':")
        print(df_train[col].value_counts(normalize=True))

        # Visualização em gráfico de barras após padronização
        plt.figure(figsize=(6, 4))
        sns.countplot(data=df_train, y=col)
        plt.title(f'Distribuição da Classe (Padr.): {col}')
        plt.xlabel('Número de Ocorrências')
        plt.ylabel('Classe')
        plt.show()
    else:
        print(f"A coluna '{col}' não foi encontrada no DataFrame.")

"""Verificando se 'tipo_do_aço_A300' e 'tipo_do_aço_A400' seguem o mesmo padrão em df_test."""

target_columns = ['tipo_do_aço_A300', 'tipo_do_aço_A400']

print("Balanceamento das classes de tipo de aço:")
for col in target_columns:
    if col in df_test.columns:
        print(f"\n--- Balanceamento para a coluna '{col}' ---")
        print(df_test[col].value_counts())
        print(f"\nProporções para '{col}':")
        print(df_test[col].value_counts(normalize=True))

        # Visualização em gráfico de barras
        plt.figure(figsize=(6, 4))
        sns.countplot(data=df_train, y=col)
        plt.title(f'Distribuição da Classe: {col}')
        plt.xlabel('Número de Ocorrências')
        plt.ylabel('Classe')
        plt.show()
    else:
        print(f"A coluna '{col}' não foi encontrada no DataFrame.")

print("Contagem de valores nulos (NaN) por coluna em df_train:")
print(df_train.isnull().sum())

empty_string_counts = {}
for col in df_train.columns:
    if df_train[col].dtype == 'object':  # Verificar apenas colunas de tipo objeto (string)
        empty_count = (df_train[col] == '').sum()
        empty_string_counts[col] = empty_count

print("\nContagem de strings vazias ('') por coluna em df_train:")
print(empty_string_counts)

df_train.info()

rows_with_missing = df_train.isnull().any(axis=1) | (df_train.astype(str) == '').any(axis=1)


print(f"Número de linhas com pelo menos um valor nulo ou string vazia: {rows_with_missing.sum()}")

df_train_cleaned = df_train[~rows_with_missing].copy()

print(f"\nNúmero de linhas em df_train após a remoção: {len(df_train_cleaned)}")

# Verificar novamente se há valores nulos ou strings vazias no DataFrame limpo
print("\nContagem de valores nulos (NaN) por coluna em df_train_cleaned:")
print(df_train_cleaned.isnull().sum())

empty_string_counts_cleaned = {}
for col in df_train_cleaned.columns:
    if df_train_cleaned[col].dtype == 'object':
        empty_count = (df_train_cleaned[col] == '').sum()
        empty_string_counts_cleaned[col] = empty_count

print("\nContagem de strings vazias ('') por coluna em df_train_cleaned:")
print(empty_string_counts_cleaned)

# O DataFrame limpo, sem linhas com NaN ou '', está agora em df_train_cleaned

#from google.colab import files
#import pandas as pd



#output_filename = 'df_train_cleaned.csv'
#df_train_cleaned.to_csv(output_filename, index=False)
#print(f"DataFrame 'df_train_cleaned' foi salvo como '{output_filename}'")

#files.download(output_filename)

#print("\nO download do arquivo foi iniciado.")

# Filtrar o DataFrame onde 'tipo_do_aço_A300' é False
filtered_df = df_train_cleaned[df_train_cleaned['tipo_do_aço_A300'] == False]

# Contar quantas vezes 'tipo_do_aço_A400' é True nesse subconjunto
count_a400_true_when_a300_false = filtered_df['tipo_do_aço_A400'].sum()

# Obter o número total de vezes que 'tipo_do_aço_A300' é False
total_a300_false = len(filtered_df)

print(f"Número de vezes em que tipo_do_aço_A300 é False e tipo_do_aço_A400 é True: {count_a400_true_when_a300_false}")
print(f"Total de vezes em que tipo_do_aço_A300 é False: {total_a300_false}")

if total_a300_false > 0:
    proportion = count_a400_true_when_a300_false / total_a300_false
    print(f"Proporção de vezes em que tipo_do_aço_A400 é True quando tipo_do_aço_A300 é False: {proportion:.2f}")
else:
    print("Não há ocorrências de tipo_do_aço_A300 sendo False no DataFrame.")

# Encontrar as linhas onde ambas as colunas são False
both_false_df = df_train_cleaned[(df_train_cleaned['tipo_do_aço_A300'] == False) & (df_train_cleaned['tipo_do_aço_A400'] == False)]

# Informar o número de ocorrências
num_both_false = len(both_false_df)
print(f"Número de linhas onde tipo_do_aço_A300 é False e tipo_do_aço_A400 é False: {num_both_false}")

# Se houver ocorrências, exibir as primeiras linhas
if not both_false_df.empty:
    print("\nPrimeiras linhas onde ambos são False:")
    print(both_false_df.head())
else:
    print("\nNão foram encontradas linhas onde ambos são False.")

# Encontrar as linhas onde ambas as colunas são True
both_true_df = df_train_cleaned[(df_train_cleaned['tipo_do_aço_A300'] == True) & (df_train_cleaned['tipo_do_aço_A400'] == True)]

# Informar o número de ocorrências
num_both_true = len(both_true_df)
print(f"\nNúmero de linhas onde tipo_do_aço_A300 é True e tipo_do_aço_A400 é True: {num_both_true}")

# Se houver ocorrências, exibir as primeiras linhas
if not both_true_df.empty:
    print("\nPrimeiras linhas onde ambos são True:")
    print(both_true_df.head())
else:
    print("\nNão foram encontradas linhas onde ambos são True.")

"""Justificativa para excluir linhas com True e True; False e False das colunas tipo_do_aço_A300 e tipo_do_aço_A400:

Para criar chapas de aço inoxidável, não se utiliza o aço A400 e o aço A300 ao mesmo tempo na mesma chapa.

Cada chapa de aço inoxidável é fabricada utilizando apenas um tipo específico de aço inoxidável. O aço inoxidável é uma liga metálica composta principalmente de ferro, cromo (que confere resistência à corrosão) e níquel, além de outros elementos em menores proporções para ajustar propriedades específicas. A não utilização simultânea de aços de carbono como A400 e A300 na fabricação de aço inoxidável é uma consequência direta da própria definição e dos requisitos de composição química para que um material seja classificado e funcione como aço inoxidável. Qualquer mistura com outros tipos de aço alteraria essas propriedades fundamentais
"""

# Condição 1: tipo_do_aço_A300 é False E tipo_do_aço_A400 é False
condition1 = (df_train_cleaned['tipo_do_aço_A300'] == False) & (df_train_cleaned['tipo_do_aço_A400'] == False)

# Condição 2: tipo_do_aço_A300 é True E tipo_do_aço_A400 é True
condition2 = (df_train_cleaned['tipo_do_aço_A300'] == True) & (df_train_cleaned['tipo_do_aço_A400'] == True)

# Criar uma máscara para as linhas que NÃO atendem a NENHUMA das condições
rows_to_keep = ~(condition1 | condition2)

# Criar um novo DataFrame contendo apenas as linhas que queremos manter
df_train_filtered = df_train_cleaned[rows_to_keep].copy()

# Verificar o número de linhas antes e depois da filtragem
print(f"Número de linhas em df_train_cleaned antes da filtragem: {len(df_train_cleaned)}")
print(f"Número de linhas em df_train_filtered após a filtragem: {len(df_train_filtered)}")

# Encontrar as linhas onde ambas as colunas são False
both_false_df = df_train_filtered[(df_train_filtered['tipo_do_aço_A300'] == False) & (df_train_filtered['tipo_do_aço_A400'] == False)]
# Encontrar as linhas onde ambas as colunas são False
both_true_df = df_train_filtered[(df_train_filtered['tipo_do_aço_A300'] == True) & (df_train_filtered['tipo_do_aço_A400'] == True)]

# Informar o número de ocorrências
num_both_false = len(both_false_df)
print(f"Número de linhas onde tipo_do_aço_A300 é False e tipo_do_aço_A400 é False: {num_both_false}")
num_both_true = len(both_true_df)
print(f"Número de linhas onde tipo_do_aço_A300 é True e tipo_do_aço_A400 é True: {num_both_true}")

def definir_tipo_de_aco(row):
    if row['tipo_do_aço_A300']:
        return 'A300'
    elif row['tipo_do_aço_A400']:
        return 'A400'
    else:
        return 'Outro'

df_train_filtered['tipo_de_aço'] = df_train_filtered.apply(definir_tipo_de_aco, axis=1)

# Verificar as primeiras linhas com a nova coluna
print(df_train_filtered[['tipo_do_aço_A300', 'tipo_do_aço_A400', 'tipo_de_aço']].head())

# Verificar a distribuição dos tipos de aço
print("\nDistribuição da nova coluna 'tipo_de_aço':")
print(df_train_filtered['tipo_de_aço'].value_counts())
print(df_train_filtered['tipo_de_aço'].value_counts(normalize=True))

df_test.info()

def definir_tipo_de_aco(row):
    if row['tipo_do_aço_A300']:
        return 'A300'
    elif row['tipo_do_aço_A400']:
        return 'A400'
    else:
        return 'Outro'

df_test['tipo_de_aço'] = df_test.apply(definir_tipo_de_aco, axis=1)

# Verificar as primeiras linhas com a nova coluna
print(df_test[['tipo_do_aço_A300', 'tipo_do_aço_A400', 'tipo_de_aço']].head())

# Verificar a distribuição dos tipos de aço
print("\nDistribuição da nova coluna 'tipo_de_aço':")
print(df_test['tipo_de_aço'].value_counts())
print(df_test['tipo_de_aço'].value_counts(normalize=True))

"""Coluna espessura_da_chapa_de_aço"""

# Contar valores negativos
negative_count = (df_train_filtered['espessura_da_chapa_de_aço'] < 0).sum()

# Contar valores iguais a zero
zero_count = (df_train_filtered['espessura_da_chapa_de_aço'] == 0).sum()

print(f"Número de valores negativos na coluna 'espessura_da_chapa_de_aço': {negative_count}")
print(f"Número de valores iguais a zero na coluna 'espessura_da_chapa_de_aço': {zero_count}")

"""Espessura de chapa de aço é uma medida física que, teoricamente, não pode ser negativa. Isso sugere que esses valores são erros de entrada de dados, problemas de medição ou algum outro tipo de anomalia."""

# Criar uma máscara booleana para selecionar as linhas onde a espessura NÃO é negativa
positive_thickness = df_train_filtered['espessura_da_chapa_de_aço'] >= 0

# Filtrar o DataFrame usando a máscara para manter apenas as linhas com espessura não negativa
df_train_filtered = df_train_filtered[positive_thickness].copy()

# Verificar o número de linhas após a remoção
print(f"Número de linhas em df_train_filtered após remover valores negativos na coluna 'espessura_da_chapa_de_aço': {len(df_train_filtered)}")

# Verificar se ainda existem valores negativos na coluna (deve ser 0)
negative_count_after_removal = (df_train_filtered['espessura_da_chapa_de_aço'] < 0).sum()
print(f"Número de valores negativos na coluna 'espessura_da_chapa_de_aço' APÓS a remoção: {negative_count_after_removal}")

"""Valores negativos"""

columns_to_check = df_train_filtered.select_dtypes(include=['number']).columns.tolist()

print("Contagem de valores negativos por coluna:")
for col in columns_to_check:
    negative_count = (df_train_filtered[col] < 0).sum()
    if negative_count > 0:
        print(f'{col}: {negative_count}')
    else:
        print(f'{col}: 0')

columns_to_check = df_test.select_dtypes(include=['number']).columns.tolist()

print("Contagem de valores negativos por coluna:")
for col in columns_to_check:
    negative_count = (df_test[col] < 0).sum()
    if negative_count > 0:
        print(f'{col}: {negative_count}')
    else:
        print(f'{col}: 0')

"""* x_minimo: Representa a menor coordenada X de uma bounding box. Coordenadas em um sistema de coordenadas padrão não são negativas para definir uma posição dentro de um espaço positivo.
* x_maximo: Representa a maior coordenada X de uma bounding box. Similar a x_minimo, coordenadas não devem ser negativas.
* y_minimo: Representa a menor coordenada Y de uma bounding box. Coordenadas Y não devem ser negativas.
* y_maximo: Representa a maior coordenada Y de uma bounding box. Coordenadas Y não devem ser negativas.
* area_pixels: Representa o número total de pixels dentro de uma região. Uma contagem não pode ser negativa.
* perimetro_x: Representa o perímetro (comprimento da borda) ao longo do eixo X. Comprimento é uma medida não negativa.
* perimetro_y: Representa o perímetro (comprimento da borda) ao longo do eixo Y. Comprimento é uma medida não negativa.
* comprimento_do_transportador: Representa o comprimento de um objeto físico. Comprimento é uma medida não negativa.
"""

columns_to_clean = ['x_minimo', 'x_maximo', 'y_minimo', 'y_maximo', 'area_pixels', 'perimetro_x', 'perimetro_y', 'comprimento_do_transportador']

print(f"Número de linhas em df_train_filtered antes da remoção: {len(df_train_filtered)}")

for col in columns_to_clean:
    # Criar uma máscara booleana para selecionar as linhas onde o valor é não negativo
    non_negative_mask = df_train_filtered[col] >= 0

    # Filtrar o DataFrame, mantendo apenas as linhas onde a máscara é True
    df_train_filtered = df_train_filtered[non_negative_mask].copy()
    print(f"Removendo valores negativos da coluna '{col}'. Número de linhas restante: {len(df_train_filtered)}")

print(f"\nNúmero de linhas em df_train_filtered após remover valores negativos das colunas especificadas: {len(df_train_filtered)}")

# Verificar se ainda existem valores negativos nas colunas especificadas (deve ser 0)
print("\nContagem de valores negativos APÓS a remoção:")
for col in columns_to_clean:
    negative_count = (df_train_filtered[col] < 0).sum()
    print(f'{col}: {negative_count}')

df_train_filtered.info()

#from google.colab import files
#import pandas as pd



#output_filename = 'df_train_filtered.csv'
#df_train_filtered.to_csv(output_filename, index=False)
#print(f"DataFrame 'df_train_filtered' foi salvo como '{output_filename}'")

#files.download(output_filename)

#print("\nO download do arquivo foi iniciado.")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



# Calcular a matriz de correlação
correlation_matrix = df_train_filtered.corr(numeric_only=True)

# Criar um mapa de calor da matriz de correlação
plt.figure(figsize=(18, 16))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Matriz de Correlação das Colunas Numéricas e Booleanas')
plt.show()


print("\nMatriz de Correlação:")
print(correlation_matrix)

"""1. Variações na Intensidade das Correlações:

    As correlações variam de -1 a +1. Valores próximos de +1 indicam uma forte correlação positiva (quando uma variável aumenta, a outra tende a aumentar também). Valores próximos de -1 indicam uma forte correlação negativa (quando uma variável aumenta, a outra tende a diminuir). Valores próximos de 0 indicam uma correlação linear fraca ou inexistente.
2. Correlações Fortes e suas Implicações:

* Correlações Positivas Fortes:

    * x_minimo e x_maximo (0.99): Fortíssima correlação positiva, o que é esperado, já que definem a extensão horizontal de um objeto. Se o x_minimo aumenta, o x_maximo também tende a aumentar, e vice-versa. Isso sugere uma alta redundância de informação entre essas duas colunas.
    * area_pixels, perimetro_x, perimetro_y, soma_da_luminosidade e seus logaritmos: Apresentam correlações positivas elevadas entre si (geralmente acima de 0.9). Isso indica que essas medidas tendem a variar juntas. Por exemplo, quanto maior a área em pixels, maior tende a ser o perímetro e a soma da luminosidade. Novamente, pode haver redundância de informação.
    * index_externo_x com area_pixels, perimetro_x, soma_da_luminosidade, log_das_areas e log_indice_x: Correlações positivas consideráveis (acima de 0.7). Isso sugere que objetos com maiores áreas e perímetros tendem a ter um índice externo x maior.
    * indice_de_bordas_y com x_minimo e x_maximo: Correlações positivas moderadas (ao redor de 0.44). Isso pode indicar que objetos com maior extensão horizontal tendem a ter um índice de bordas y maior.
    * tipo_do_aço_A300 e tipo_do_aço_A400: Correlação negativa perfeita (-1.0). Isso é esperado, pois são variáveis mutuamente exclusivas (se o tipo de aço é A300, não pode ser A400 e vice-versa).


* Correlações Negativas Fortes:

    * area_pixels, perimetro_x, perimetro_y, soma_da_luminosidade e seus logaritmos com index_vazio: Correlações negativas moderadas a fortes (geralmente abaixo de -0.3). Isso sugere que quanto maior a área/perímetro/luminosidade, menor tende a ser o índice de vazio.
    * indice_de_bordas_y com y_minimo: Correlação negativa moderada (-0.40). Isso pode indicar que quanto maior o y_minimo, menor tende a ser o índice de bordas y.
3. Correlações Fracas ou Próximas de Zero:

    * Muitas variáveis apresentam correlações fracas (próximas de zero)
    As correlações entre as coordenadas (x_minimo, x_maximo, y_minimo, y_maximo) e as variáveis de defeito (falha_1 a falha_outros) geralmente são fracas, indicando que a localização da placa não é um forte preditor para a ocorrência dessas falhas, pelo menos linearmente.
    * A variável peso_da_placa possui valor NaN em todas as correlações, o que indica que todos os valores dessa coluna são provavelmente nulos, e portanto, não há correlação a ser calculada.
"""



"""## 05 Modelagem

### 5.1 Selecionar um modelo de classificação simples "Regressão Logística".
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, multilabel_confusion_matrix
import numpy as np

"""### 5.2 Preparar e Dividir os dados em conjuntos de treino e teste."""

df = df_train_filtered

# Definir colunas alvo (falhas)
target_columns = ['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros']

# Definir colunas de features (preditoras)
# Vamos usar todas as colunas exceto 'id', 'tipo_de_aço' (redundante com A300/A400) e as colunas alvo
feature_columns = [col for col in df.columns if col not in ['id', 'tipo_de_aço'] + target_columns]

# Separar features (X) e alvos (y)
X = df[feature_columns]
y = df[target_columns]

# Converter colunas booleanas em features para numérico (0 ou 1)
# Scikit-learn geralmente lida bem com booleanos, mas a conversão explícita é mais segura
bool_feature_cols = X.select_dtypes(include='bool').columns
X[bool_feature_cols] = X[bool_feature_cols].astype(int)

# Converter colunas alvo booleanas para numérico (0 ou 1)
y = y.astype(int)

# Verificar se há colunas não numéricas restantes em X (exceto as booleanas já convertidas)
non_numeric_cols = X.select_dtypes(exclude=np.number).columns
if len(non_numeric_cols) > 0:
    print(f"Atenção: Colunas não numéricas encontradas em X: {non_numeric_cols.tolist()}")

# Dividir os dados em conjuntos de treino e teste
# stratify=y pode ser útil em classificação multi-label, mas é mais complexo.
# Para um modelo simples, vamos omitir a estratificação por enquanto.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Padronizar/Escalar as features numéricas
# É importante para a Regressão Logística
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test) # Usar o mesmo scaler ajustado no treino

# Converter de volta para DataFrame para manter nomes das colunas (opcional, mas útil)
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

"""### 5.3 Treinar o modelo selecionado."""

# --- 3. Treinamento do Modelo (One-vs-Rest) ---

models = {} # Dicionário para guardar um modelo para cada tipo de falha
predictions = {} # Dicionário para guardar as previsões para cada tipo de falha

print("Iniciando treinamento dos modelos...")

for target in target_columns:
    print(f"--- Treinando modelo para: {target} ---")

    # Verificar se há variância suficiente na coluna alvo no treino
    if len(y_train[target].unique()) < 2:
        print(f"Aviso: A coluna alvo '{target}' tem apenas um valor no conjunto de treino. Pulando treinamento.")
        models[target] = None # Marcar como não treinado
        predictions[target] = np.zeros(len(y_test)) # Prever tudo como classe majoritária (0)
        continue # Pular para a próxima falha

    # Criar e treinar o modelo de Regressão Logística
    # class_weight='balanced' ajuda se as classes (falha/não falha) forem desbalanceadas
    model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced')
    model.fit(X_train_scaled, y_train[target])

    # Guardar o modelo treinado
    models[target] = model

    # Fazer previsões no conjunto de teste
    y_pred = model.predict(X_test_scaled)
    predictions[target] = y_pred

print("\nTreinamento concluído.")

"""### 5.4 Avaliar o desempenho do modelo (acurácia, precisão, recall, F1-score, etc.)."""

# --- 4. Avaliação dos Modelos ---

print("\n--- Avaliação dos Modelos no Conjunto de Teste ---")

# DataFrame para armazenar as previsões lado a lado com os valores reais
results_df = y_test.copy()
for target in target_columns:
    if models[target] is not None: # Apenas para modelos que foram treinados
      results_df[f'{target}_pred'] = predictions[target]

# Avaliar cada modelo individualmente
for target in target_columns:
    print(f"\n--- Avaliação para: {target} ---")
    if models[target] is None:
        print("Modelo não treinado devido à falta de variância nos dados de treino.")
        continue

    y_true_col = y_test[target]
    y_pred_col = predictions[target]

    # Métricas
    accuracy = accuracy_score(y_true_col, y_pred_col)
    print(f"Acurácia: {accuracy:.4f}")

    # Relatório de Classificação (Precisão, Recall, F1-score)
    # zero_division=0 evita warnings se uma classe não tiver previsões
    print("Relatório de Classificação:")
    print(classification_report(y_true_col, y_pred_col, zero_division=0))

    # Matriz de Confusão
    print("Matriz de Confusão:")
    # Formato: [[TN, FP], [FN, TP]]
    # TN: Verdadeiro Negativo (Não Falha, Previsto Não Falha)
    # FP: Falso Positivo (Não Falha, Previsto Falha)
    # FN: Falso Negativo (Falha, Previsto Não Falha)
    # TP: Verdadeiro Positivo (Falha, Previsto Falha)
    print(confusion_matrix(y_true_col, y_pred_col))

"""### 5.5 Implementar uma função simples para realizar predições com probabilidades."""

df_novo = df_test[feature_columns].copy()



# --- 2. Pré-processamento do Novo Conjunto de Dados ---
print("Pré-processando o novo conjunto de dados...")

# Certifique-se de que o novo DataFrame tem todas as colunas de features necessárias
# e na ordem correta, se o scaler for sensível à ordem (StandardScaler não é, mas é boa prática)
missing_cols = set(feature_columns) - set(df_novo.columns)
if missing_cols:
    raise ValueError(f"As seguintes colunas de features estão faltando no novo DataFrame: {missing_cols}")

# Selecionar apenas as colunas de features
X_novo = df_novo[feature_columns].copy() # Use .copy() para evitar SettingWithCopyWarning

# Converter colunas booleanas em features para numérico (0 ou 1), se houver
bool_feature_cols_novo = X_novo.select_dtypes(include='bool').columns
if not bool_feature_cols_novo.empty:
    X_novo[bool_feature_cols_novo] = X_novo[bool_feature_cols_novo].astype(int)

# Verificar se há colunas não numéricas restantes em X_novo (deveriam ter sido tratadas)
non_numeric_cols_novo = X_novo.select_dtypes(exclude=np.number).columns
if len(non_numeric_cols_novo) > 0:
    print(f"Atenção: Colunas não numéricas encontradas em X_novo: {non_numeric_cols_novo.tolist()}")
    # Você precisaria aplicar o mesmo tratamento que fez no treino (ex: one-hot encoding)
    # Se você usou pd.get_dummies no treino, precisará aplicá-lo aqui também,
    # garantindo que as colunas resultantes sejam as mesmas (pode precisar de reindex).
    # Para este exemplo, assumimos que as features já estão prontas ou são apenas booleanas/numéricas.

# Aplicar o MESMO scaler que foi ajustado nos dados de TREINO
# NÃO FAÇA scaler.fit(X_novo) ou scaler.fit_transform(X_novo) AQUI!
try:
    X_novo_scaled = scaler.transform(X_novo)
except Exception as e:
    print(f"Erro ao aplicar o scaler: {e}")
    print("Verifique se X_novo tem as mesmas colunas e tipos de dados que os dados de treinamento (antes do scaling).")
    # Adicione aqui uma saída das colunas de X_novo e X_train (do script anterior) para depuração
    # print("Colunas em X_novo:", X_novo.columns.tolist())
    # print("Tipos de dados em X_novo:\n", X_novo.dtypes)
    # print("Número de features esperadas pelo scaler:", scaler.n_features_in_)
    raise

# Converter de volta para DataFrame para manter nomes das colunas (opcional, mas pode ser útil)
X_novo_scaled_df = pd.DataFrame(X_novo_scaled, columns=X_novo.columns, index=X_novo.index)


# --- 3. Fazer Previsões no Novo Conjunto de Dados ---
print("Fazendo previsões...")
novo_predictions_dict = {}

for target in target_columns:
    if models.get(target) is not None: # Verifica se o modelo para este alvo existe e foi treinado
        model_para_alvo = models[target]
        predicoes_para_alvo = model_para_alvo.predict(X_novo_scaled_df) # ou X_novo_scaled
        novo_predictions_dict[f'{target}_pred'] = predicoes_para_alvo
    else:
        print(f"Aviso: Modelo para '{target}' não encontrado ou não treinado. Preenchendo com NaN ou 0.")
        # Você pode decidir como lidar com isso: preencher com NaN, 0, ou levantar um erro.
        # Preencher com 0 (sem falha) pode ser uma suposição razoável se o modelo não pôde ser treinado.
        novo_predictions_dict[f'{target}_pred'] = np.zeros(len(X_novo_scaled_df), dtype=int)


# --- 4. Formatar e Exibir/Salvar as Previsões ---
# Converter o dicionário de previsões em um DataFrame
df_novo_predictions = pd.DataFrame(novo_predictions_dict, index=df_novo.index)

# Opcional: Concatenar as previsões com o DataFrame original (ou com colunas selecionadas como 'id')
# Se df_novo tiver uma coluna 'id', podemos usá-la.
if 'id' in df_novo.columns:
    df_resultados_finais = pd.concat([df_novo[['id']], df_novo_predictions], axis=1)
else:
    df_resultados_finais = df_novo_predictions.copy() # Ou adicione um índice resetado se necessário

print("\n--- Previsões no Novo Conjunto de Dados ---")
print(df_resultados_finais.head())

# Você pode querer salvar essas previsões em um arquivo CSV
# df_resultados_finais.to_csv('previsoes_novos_dados.csv', index=False)

# Exemplo de como acessar as previsões para uma falha específica:
# print("\nPrevisões para falha_1:")
# print(df_resultados_finais[['id', 'falha_1_pred']].head() if 'id' in df_resultados_finais else df_resultados_finais['falha_1_pred'].head())

df_resultados_finais

"""## 06 Engenharia de Features

### 6.1 Criar novas variáveis (features) a partir dos dados existentes ou transformar as variáveis existentes
"""

df_train_filtered.info()

"""A análise de correlação revelou que diversas variáveis apresentam relações lineares fracas (próximas de zero), especialmente entre as coordenadas da placa (x_minimo, x_maximo, y_minimo, y_maximo) e os tipos de defeito (falha_1 a falha_outros), sugerindo que a localização, por si só, não se configura como um preditor linear significativo para a ocorrência dessas falhas. Adicionalmente, a variável peso_da_placa demonstrou ausência de correlação em todas as análises, indicando uma provável completude de valores nulos, o que a torna inutilizável para modelagem preditiva baseada em correlação. Para as variáveis 'tipo_do_aço_A300' e 'tipo_do_aço_A400', foi criada uma nova variável chamada 'tipo de aço'."""

colunas_para_remover = ['id', 'peso_da_placa', 'x_minimo', 'x_maximo', 'y_minimo', 'y_maximo', 'tipo_do_aço_A300', 'tipo_do_aço_A400']
df_features = df_train_filtered.drop(columns=colunas_para_remover)

df_features.info()

"""Após a análise dos dados, identificou-se a existência de dois tipos distintos de aço, denominados A300 e A400, como categorias relevantes dentro do conjunto de dados. Diante dessa distinção fundamental, a criação de DataFrames separados para cada tipo de aço (df_aco_A300 e df_aco_A400) se justifica para facilitar análises mais específicas e a modelagem preditiva individualizada para cada tipo de produto. Essa separação permitirá investigar padrões, características e comportamentos distintos associados a cada tipo de aço, potencialmente revelando insights mais precisos e relevantes para o negócio."""

# Criando o DataFrame para o tipo de aço A300
df_aco_A300 = df_features[df_features['tipo_de_aço'] == 'A300'].copy()
df_aco_A300 = df_aco_A300.drop(columns=['tipo_de_aço'])

# Criando o DataFrame para o tipo de aço A400
df_aco_A400 = df_features[df_features['tipo_de_aço'] == 'A400'].copy()
df_aco_A400 = df_aco_A400.drop(columns=['tipo_de_aço'])

colunas_sem_dados = df_aco_A300.columns[df_aco_A300.isnull().all()]

if not colunas_sem_dados.empty:
    print("As seguintes colunas não possuem dados (todos os valores são nulos):")
    print(colunas_sem_dados.tolist())
else:
    print("Não há colunas sem dados (com todos os valores nulos) no df_aco_A300.")

df_aco_A300.info()

colunas_sem_dados = df_aco_A300.columns[df_aco_A400.isnull().all()]

if not colunas_sem_dados.empty:
    print("As seguintes colunas não possuem dados (todos os valores são nulos):")
    print(colunas_sem_dados.tolist())
else:
    print("Não há colunas sem dados (com todos os valores nulos) no df_aco_A400.")

df_aco_A400.info()

"""## 7 Definição de Modelos"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.multioutput import ClassifierChain
from sklearn.metrics import classification_report, hamming_loss, accuracy_score

# Separando as colunas de características (X) e as colunas alvo (y)
X = df_aco_A300.drop(columns=['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros'])
y = df_aco_A300[['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros']].astype(int) # Convertendo bool para int (0 ou 1)

# Dividindo os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 1. Modelo de Árvore de Decisão para Multirrótulo
decision_tree = MultiOutputClassifier(DecisionTreeClassifier(random_state=42))
decision_tree.fit(X_train, y_train)
y_pred_dt = decision_tree.predict(X_test)
y_prob_dt = decision_tree.predict_proba(X_test) # predict_proba retorna uma lista de arrays de probabilidades para cada rótulo

print("Resultados da Árvore de Decisão (Multirrótulo):\n")
print("Classification Report:\n", classification_report(y_test, y_pred_dt))
print("Hamming Loss:", hamming_loss(y_test, y_pred_dt))
print("Accuracy (amostra completa):", accuracy_score(y_test, y_pred_dt))
#print("\nProbabilidades para a primeira amostra no conjunto de teste (para cada falha):\n", [prob[:, 1] for prob in y_prob_dt])

# 2. Modelo de Random Forest para Multirrótulo
random_forest = MultiOutputClassifier(RandomForestClassifier(random_state=42))
random_forest.fit(X_train, y_train)
y_pred_rf = random_forest.predict(X_test)
y_prob_rf = random_forest.predict_proba(X_test) # predict_proba retorna uma lista de arrays de probabilidades para cada rótulo

print("\nResultados da Random Forest (Multirrótulo):\n")
print("Classification Report:\n", classification_report(y_test, y_pred_rf))
print("Hamming Loss:", hamming_loss(y_test, y_pred_rf))
print("Accuracy (amostra completa):", accuracy_score(y_test, y_pred_rf))
#print("\nProbabilidades para a primeira amostra no conjunto de teste (para cada falha):\n", [prob[:, 1] for prob in y_prob_rf])

# 3. Modelo de Gradient Boosting para Multirrótulo
gradient_boosting = MultiOutputClassifier(GradientBoostingClassifier(random_state=42))
gradient_boosting.fit(X_train, y_train)
y_pred_gb = gradient_boosting.predict(X_test)
# GradientBoostingClassifier não possui predict_proba diretamente dentro do MultiOutputClassifier
# Para obter probabilidades, você precisaria acessar o predict_proba de cada classificador individualmente.

print("\nResultados do Gradient Boosting (Multirrótulo):\n")
print("Classification Report:\n", classification_report(y_test, y_pred_gb))
print("Hamming Loss:", hamming_loss(y_test, y_pred_gb))
print("Accuracy (amostra completa):", accuracy_score(y_test, y_pred_gb))
# print("\nProbabilidades para a primeira amostra no conjunto de teste (para cada falha):\n", [prob[:, 1] for prob in y_prob_gb])

# 4. Algoritmo de Cadeias de Classificadores (Classifier Chains)
# Definindo a ordem das colunas alvo (a ordem pode ser importante)
target_columns = ['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros']
chain = ClassifierChain(GradientBoostingClassifier(random_state=42), order='random', random_state=42) # Você pode definir uma ordem específica ou usar 'random'
chain.fit(X_train, y_train)
y_pred_chain = chain.predict(X_test)
y_prob_chain = chain.predict_proba(X_test) # predict_proba retorna as probabilidades diretamente para a cadeia

print("\nResultados do Classifier Chains (Gradient Boosting):\n")
print("Classification Report:\n", classification_report(y_test, y_pred_chain))
print("Hamming Loss:", hamming_loss(y_test, y_pred_chain))
print("Accuracy (amostra completa):", accuracy_score(y_test, y_pred_chain))
#print("\nProbabilidades para a primeira amostra no conjunto de teste (para cada falha):\n", y_prob_chain[0])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import classification_report, hamming_loss, accuracy_score
import numpy as np

# Separando as colunas de características (X) e as colunas alvo (y)
X = df_aco_A300.drop(columns=['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros'])
y = df_aco_A300[['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros']].astype(int)

# Dividindo os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Pré-processamento das features numéricas (padronização)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Definindo a arquitetura da rede neural
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(32, activation='relu'),
    Dense(y_train.shape[1], activation='sigmoid') # Camada de saída com uma unidade por falha e ativação sigmoid
])

# Compilando o modelo
model.compile(optimizer='adam',
              loss='binary_crossentropy', # Função de perda para classificação multirrótulo
              metrics=['accuracy']) # A acurácia aqui é por instância (todos os rótulos corretos)

# Treinando o modelo
epochs = 50
batch_size = 32
history = model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=0)

# Fazendo previsões (as saídas serão probabilidades)
y_prob_nn = model.predict(X_test_scaled)

# Convertendo probabilidades em rótulos binários (usando um limiar de 0.5)
y_pred_nn = (y_prob_nn > 0.5).astype(int)

# Avaliando o modelo
print("\nResultados da Rede Neural (Keras):\n")
print("Classification Report:\n", classification_report(y_test, y_pred_nn))
print("Hamming Loss:", hamming_loss(y_test, y_pred_nn))
print("Accuracy (amostra completa):", accuracy_score(y_test, y_pred_nn))
#print("\nProbabilidades para a primeira amostra no conjunto de teste (para cada falha):\n", y_prob_nn[0])

def analisar_coluna_falha(df, nome_coluna):
    """
    Conta e analisa os valores True e False em uma coluna booleana de um DataFrame.

    Args:
        df (pd.DataFrame): O DataFrame a ser analisado.
        nome_coluna (str): O nome da coluna booleana.
    """
    if nome_coluna not in df.columns:
        print(f"Erro: A coluna '{nome_coluna}' não existe no DataFrame.")
        return

    coluna = df[nome_coluna]

    if not pd.api.types.is_bool_dtype(coluna):
        print(f"Aviso: A coluna '{nome_coluna}' não é do tipo booleano. Os resultados podem não ser representativos.")

    contagem = coluna.value_counts(dropna=False) # Inclui a contagem de valores NaN, se houver

    print(f"Análise da coluna '{nome_coluna}':")
    if True in contagem:
        print(f"  True: {contagem[True]}")
    else:
        print("  True: 0")

    if False in contagem:
        print(f"  False: {contagem[False]}")
    else:
        print("  False: 0")

    if pd.isna(contagem.index).any():
        print(f"  NaN: {contagem[pd.isna(contagem.index)].iloc[0]}")

# Exemplo de uso:
#analisar_coluna_falha(df_aco_A300, 'falha_3')

analisar_coluna_falha(df_aco_A300, 'falha_1')
analisar_coluna_falha(df_aco_A300, 'falha_2')
analisar_coluna_falha(df_aco_A300, 'falha_3')
analisar_coluna_falha(df_aco_A300, 'falha_4')
analisar_coluna_falha(df_aco_A300, 'falha_5')
analisar_coluna_falha(df_aco_A300, 'falha_6')
analisar_coluna_falha(df_aco_A300, 'falha_outros')

"""(Desbalanceamento de Classes do df_aco_A300):

falha_3, falha_4 e falha_5: Estas colunas apresentam um desbalanceamento extremo. Com apenas 3, 1 e 4 ocorrências de True, respectivamente, o modelo terá muita dificuldade em aprender os padrões característicos dessas falhas. Modelos podem tender a prever sempre False para essas classes para otimizar a acurácia geral.
falha_1: Também apresenta um desbalanceamento considerável (28 True vs. 480 False).
falha_2: Possui um desbalanceamento menor, mas ainda significativo (86 True vs. 422 False).
falha_6 e falha_outros: Apresentam o desbalanceamento menos severo entre as colunas de falha, com um número razoável de ocorrências de ambas as classes.

Técnicas de Reamostragem:

* Oversampling: Duplicar ou gerar amostras sintéticas da classe minoritária (e.g., SMOTE).
* Undersampling: Remover amostras da classe majoritária.
"""

# Remover amostras da classe majoritária.

# 4. Algoritmo de Cadeias de Classificadores (Classifier Chains)
# Definindo a ordem das colunas alvo (a ordem pode ser importante)
#target_columns = ['falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros']
target_columns = ['falha_6', 'falha_outros']
chain = ClassifierChain(GradientBoostingClassifier(random_state=42), order='random', random_state=42) # Você pode definir uma ordem específica ou usar 'random'
chain.fit(X_train, y_train)
y_pred_chain = chain.predict(X_test)
y_prob_chain = chain.predict_proba(X_test) # predict_proba retorna as probabilidades diretamente para a cadeia

print("\nResultados do Classifier Chains (Gradient Boosting):\n")
print("Classification Report:\n", classification_report(y_test, y_pred_chain))
print("Hamming Loss:", hamming_loss(y_test, y_pred_chain))
print("Accuracy (amostra completa):", accuracy_score(y_test, y_pred_chain))
#print("\nProbabilidades para a primeira amostra no conjunto de teste (para cada falha):\n", y_prob_chain[0])

"""df_aco_A400"""

analisar_coluna_falha(df_aco_A400, 'falha_1')
analisar_coluna_falha(df_aco_A400, 'falha_2')
analisar_coluna_falha(df_aco_A400, 'falha_3')
analisar_coluna_falha(df_aco_A400, 'falha_4')
analisar_coluna_falha(df_aco_A400, 'falha_5')
analisar_coluna_falha(df_aco_A400, 'falha_6')
analisar_coluna_falha(df_aco_A400, 'falha_outros')

# Separando as colunas de características (X) e as colunas alvo (y)
X = df_aco_A400.drop(columns=['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros'])
y = df_aco_A400[['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros']].astype(int)

# Dividindo os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(y_train.columns.tolist())

print(y_test.columns.tolist())

# 1. Algoritmo de Cadeias de Classificadores (Classifier Chains)
# Definindo a ordem das colunas alvo (a ordem pode ser importante)
target_columns_ordered = ['falha_1', 'falha_2', 'falha_3', 'falha_4', 'falha_5', 'falha_6', 'falha_outros']

chain = ClassifierChain(GradientBoostingClassifier(random_state=42), order='random', random_state=42) # Você pode definir uma ordem específica ou usar 'random'
chain.fit(X_train, y_train)
y_pred_chain = chain.predict(X_test)
y_prob_chain = chain.predict_proba(X_test) # predict_proba retorna as probabilidades diretamente para a cadeia

print("\nResultados do Classifier Chains (Gradient Boosting):\n")
print("Classification Report:\n", classification_report(y_test, y_pred_chain))
print("Hamming Loss:", hamming_loss(y_test, y_pred_chain))
print("Accuracy (amostra completa):", accuracy_score(y_test, y_pred_chain))
#print("\nProbabilidades para a primeira amostra no conjunto de teste (para cada falha):\n", y_prob_chain[0])

"""Otimização dos Hiperparâmetros do Gradient Boosting

O GradientBoostingClassifier possui vários hiperparâmetros que podem ser ajustados para melhorar o desempenho. Você pode usar técnicas de busca como GridSearchCV ou RandomizedSearchCV para encontrar a melhor combinação de parâmetros. Alguns hiperparâmetros importantes para otimizar incluem:

n_estimators: Número de árvores de boosting. Aumentar pode melhorar o desempenho, mas também pode levar a overfitting.
learning_rate: Peso aplicado a cada árvore. Valores menores geralmente exigem mais árvores, mas podem levar a um aprendizado mais robusto.
max_depth: Profundidade máxima de cada árvore. Controla a complexidade do modelo.
min_samples_split: Número mínimo de amostras necessárias para dividir um nó interno.
min_samples_leaf: Número mínimo de amostras necessárias1 em um nó folha
subsample: Fração de amostras a serem usadas para ajustar as árvores base. Reduz a variância e pode prevenir overfitting.

### Documentação Breve dos Modelos e Principais Insights

Um resumo dos resultados dos modelos de classificação multirrótulo que avaliei, destacando os principais insights de cada um:

### 1. Árvore de Decisão (Multirrótulo):

*   **Desempenho Geral:** Este modelo apresentou o pior desempenho geral entre os avaliados, com baixa precisão, recall e f1-score médios (micro, macro e ponderado).
*   **Desempenho por Classe:** Demonstrou bom desempenho apenas para a classe '1', com alta precisão e recall. Para as demais classes, o desempenho foi muito baixo ou nulo, indicando dificuldade em prever essas categorias.
*   **Métricas Específicas:** O Hamming Loss foi relativamente alto (0.13), indicando uma frequência considerável de rótulos incorretamente previstos. A acurácia da amostra completa (0.39) também foi a mais baixa entre os modelos.
*   **Insight Principal:** Concluí que a Árvore de Decisão, em sua configuração padrão, mostrou-se um modelo inadequado para este problema de classificação multirrótulo, possivelmente devido à complexidade das relações entre as características e os múltiplos rótulos.

### 2. Random Forest (Multirrótulo):

*   **Desempenho Geral:** Observei uma melhora significativa em relação à Árvore de Decisão, com métricas médias superiores. O micro avg de f1-score (0.56) indicou um desempenho razoável na previsão de rótulos individuais.
*   **Desempenho por Classe:** Manteve um bom desempenho para a classe '1', mas ainda apresentou dificuldades significativas para as classes com menor suporte (0, 2, 4). O desempenho para as classes '5' e '6' foi moderado.
*   **Métricas Específicas:** O Hamming Loss (0.11) foi menor que o da Árvore de Decisão, indicando menos erros na previsão de rótulos. A acurácia da amostra completa (0.46) também foi superior.
*   **Insight Principal:** A Random Forest demonstrou ser mais robusta que a Árvore de Decisão para este problema, provavelmente devido à sua capacidade de agregar múltiplos classificadores e reduzir o overfitting. No entanto, ainda enfrentei desafios com classes menos representadas.

### 3. Gradient Boosting (Multirrótulo):

*   **Desempenho Geral:** Este modelo apresentou um desempenho geral similar ao da Random Forest, com métricas médias ligeiramente inferiores ou equivalentes.
*   **Desempenho por Classe:** Similar à Random Forest, mostrou bom desempenho para a classe '1' e dificuldades para as classes minoritárias. O desempenho para as classes '5' e '6' foi moderado.
*   **Métricas Específicas:** O Hamming Loss (0.11) foi comparável ao da Random Forest. A acurácia da amostra completa (0.46) foi ligeiramente superior à da Random Forest.
*   **Insight Principal:** O Gradient Boosting também se mostrou uma abordagem promissora, com resultados competitivos em relação à Random Forest. Identifiquei que ajustes nos hiperparâmetros podem potencialmente levar a melhorias.

### 4. Classifier Chains (Gradient Boosting):

*   **Desempenho Geral:** Este modelo demonstrou o melhor desempenho geral entre os avaliados, com o maior micro avg de f1-score (0.61) e a maior acurácia da amostra completa (0.61). O samples avg de f1-score também foi o mais alto (0.61).
*   **Desempenho por Classe:** Manteve o bom desempenho para a classe '1' e apresentou uma melhora notável no recall da classe '5'. O desempenho para as classes minoritárias (0, 2, 4) continuou sendo baixo.
*   **Métricas Específicas:** O Hamming Loss (0.11) foi similar aos modelos de Random Forest e Gradient Boosting, mas a acurácia da amostra completa foi significativamente maior.
*   **Insight Principal:** Concluí que a abordagem de Classifier Chains, utilizando o Gradient Boosting como classificador base, pareceu ser a mais eficaz para capturar as dependências entre os rótulos neste problema, resultando em um desempenho superior.

### 5. Rede Neural (Keras):

*   **Desempenho Geral:** Apresentou um desempenho geral competitivo, com métricas médias similares ou ligeiramente inferiores ao Classifier Chains, mas superiores aos modelos de Random Forest e Gradient Boosting individuais.
*   **Desempenho por Classe:** Mostrou um desempenho excelente para a classe '1' (alta precisão e recall) e uma melhora no recall da classe '5'. Similar aos outros modelos, enfrentei dificuldades com as classes minoritárias.
*   **Métricas Específicas:** O Hamming Loss (0.10) foi o mais baixo entre todos os modelos, indicando a menor taxa de erros na previsão de rótulos individuais. A acurácia da amostra completa (0.52) foi a segunda melhor.
*   **Insight Principal:** A Rede Neural demonstrou um grande potencial para este problema de classificação multirrótulo, apresentando o menor Hamming Loss. Um ajuste mais fino da arquitetura e dos hiperparâmetros pode levar a um desempenho ainda melhor.

### Conclusão Geral:

Observei que o modelo de Classifier Chains com Gradient Boosting e a Rede Neural (Keras) apresentaram os melhores resultados gerais. Isso sugeriu que a modelagem das dependências entre os rótulos (no caso do Classifier Chains) e a capacidade de aprendizado de representações complexas (no caso da Rede Neural) são importantes para este problema. Os modelos de Árvore de Decisão e, em menor grau, Random Forest e Gradient Boosting individuais, tiveram mais dificuldades em lidar com a natureza multirrótulo da tarefa e com as classes menos representadas. Sugeri que investigações futuras se concentrem na otimização dos hiperparâmetros dos modelos mais promissores e na exploração de técnicas para lidar com o desbalanceamento de classes.

---

## 08 Processo Realizado .

1.  **Planejamento Inicial:** Iniciei o projeto com o planejamento, criando uma lista geral de tarefas para organizar as atividades do desafio.
2.  **Definição dos Objetivos do Projeto:** Defini os objetivos do projeto, como desenvolver um sistema inteligente para classificar defeitos em chapas de aço, prever a classe do defeito com probabilidade, extrair insights e gerar visualizações.
3.  **Identificação das Necessidades da Empresa:** Identifiquei as necessidades da empresa, incluindo a automatização do controle de qualidade, detecção precoce e classificação precisa de defeitos, e a geração de informações para tomada de decisão.
4.  **Compreensão da Estrutura dos Dados:** Entendi os dados, analisando a estrutura e o significado de cada uma das 31 colunas (features) fornecidas.
5.  **Análise Exploratória Inicial:** Explorei estatísticas descritivas básicas, identificando outliers, valores ausentes e a necessidade de padronização em algumas colunas.
6.  **Verificação do Balanceamento e Consistência:** Verifiquei o balanceamento das classes de defeito e a consistência dos dados, notando a necessidade de padronizar representações de valores booleanos (ex: 'true', 'sim').
7.  **Pré-processamento e Limpeza de Dados:** Realizei o pré-processamento dos dados, tratando valores ausentes e removendo 418 linhas com dados faltantes.
8.  **Padronização e Criação de Novas Features:** Padronizei as colunas 'tipo_do_aço_A300' e 'tipo_do_aço_A400', criando a nova coluna 'tipo_de_aço' para unificar essa informação.
9.  **Tratamento de Dados Inconsistentes:** Removi valores inconsistentes, como espessuras negativas e coordenadas negativas (x_minimo, x_maximo, y_minimo, y_maximo), que não são fisicamente possíveis.
10. **Correção Adicional de Dados Numéricos:** Tratei também valores negativos em 'area_pixels', 'perimetro_x', 'perimetro_y' e 'comprimento_do_transportador'.
11. **Análise de Correlação:** Analisei a correlação entre as variáveis, identificando relações fortes, negativas, fracas e a ausência de correlação para 'peso_da_placa' devido a valores nulos.
12. **Modelagem Inicial (Baseline):** Para a modelagem inicial, selecionei e treinei um modelo de Regressão Logística como baseline.
13. **Engenharia de Features Baseada em Insights:** Na engenharia de features, utilizei insights da análise de correlação, como a criação da variável 'tipo_de_aço' e a observação da baixa preditividade linear das coordenadas.
14. **Segmentação dos Dados por Tipo de Aço:** Decidi separar os dados em DataFrames distintos para o aço A300 e A400, visando análises e modelagem específicas por tipo de produto.
15. **Exploração de Modelos Multirrótulo:** Explorei diferentes modelos de classificação multirrótulo, como Árvore de Decisão, Random Forest, Gradient Boosting e Algoritmo de Cadeias de Classificadores.
16. **Identificação do Desbalanceamento de Classes:** Identifiquei um desbalanceamento extremo de classes para algumas falhas no DataFrame do aço A300 (especialmente falha_3, falha_4, falha_5), representando um desafio para o treinamento dos modelos.
17. **Seleção do Modelo Final:** Selecionei o Algoritmo de Cadeias de Classificadores por apresentar o melhor desempenho entre os modelos multirrótulo testados.
18. **Consideração sobre Redes Neurais:** Considerei o uso de Redes Neurais (Keras) como uma possibilidade, embora não tenha sido o foco principal da modelagem final.
19. **Sugestão de Próximos Passos:** Sugeri, como próximo passo, a otimização de hiperparâmetros do modelo selecionado (e do Gradient Boosting) para potencializar os resultados, uma etapa que não foi realizada.
"""